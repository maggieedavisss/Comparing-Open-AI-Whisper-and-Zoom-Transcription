Hi, everyone. So thank you very much for watching this video for the machine learning course. So for this lecture, I'm just going to be light and have a lab section that I hope this lab session can help you to finish the second homework. So in this lab section, we're going to learn how we can implement the success selection methods and regularization methods in Python. So the first part is we're going to see how we can implement the best success selection, forward selection, and backward selection in Python. Okay. So first, we're going to import the NumPy pandas and the matplotlib package. And then for this exercise, we're going to use the hitters data where we want to predict a baseball player's salary on the basis of various statistics associated with their performance in their previous year. So to start, let's first look at their data so we can load the data from the ISLP package. So what we want to do is we want to predict the salary based on some other variables. And then in this data set, we can see that for some of the players, their salary is missing. So what we want to do first is to count how many players with missing salary. And there are 59 in total. So what we want to do is we want to clean up the data set a bit and remove the players where we do not have their salary information. So the way to do that is we drop the players without salary information. And then by using that, we have this hitters clean data set. And then after we do that, we can see that we do not have any more players without salary in this clean data. And then the next is to construct the data frame of predictors and the pandas time series for their outcome, the salary. So the way to do that is we use the SM from the StatsModel API package. And then you may have noticed that in their original data, some of the variables are categorical like league, division, and new league. So we first want to get the dummy variables from this categorical data. And then we extract their salary as their outcome. So for their x, we drop their categorical variables and their outcome salary. And then we convert the data type to float. And then we concatenate their x with their dummy variables of those categorical variables, and we get their data frame of all the predictors. So we can see that there are 19 predictors in total. And then the next is to conduct their best subset selection. And then to do that, we write two functions. The first function is what we call as the process subset. So what this function does is based on the names of their features, we construct a subset of x with the subset of the feature names. And then we pass that into this StatsOLS. So we predict y based on the subset of their features in x. And then we fit the model. After we fit the model, we calculate the residual sum of squares. And then we return their model prediction, the model and the residual sum of squares. So that is the first function. The next function is for each number of predictors k, we want to identify the best k predictors. So we write a function to do that. And then this function is what we call as getBest. So here we count how long it should take to get the best k predictor model. And we compare this against the forward and backward selection next. And then we can see that the best subset selection is more time consuming. And then for this part, we use a function where we call it as the eatHerTruthDocumentations. So basically, this could identify all the k variables from their 19 variables in total. And then based on that, so we get all the possible k feature names, we then pass that into the process subset to fit the model using the subset features. And then we get the model and the residual sum of squares and append that to the result. Then we wrap everything up into a nice data frame. We choose the model with the minimum residual sum of squares. And then last is to count how long it takes to get the best k predictor model. And then the next is we iterate the number of predictors from 1 to 7. And now we get the best 1 to best 7 predictor model. So here, I'm not going to run this cell again because it can take a while to get the best 5 to 7 predictor models. But you can see from the results here that if we want to get the best 5, 6, or 7 predictor model, we need to process to fit more than 10k, 27k, and 50k models to get the best 6, 7, and 8 predictor models. And then it can take up to a minute to finish running. So it's definitely very time-consuming. And then after we run this cell, we get the model best. And then here, the first column is the best residual sum of squares for the best 1 predictor, 2 predictors, 3 predictor, up to 7 predictor model. And then the second column here saves the best model, best fitting model. So to make this more concrete, we can look at what is the best 2 predictor model. So here, in this model's best, we locate the best 2 predictor model. And then we output the summary table. So we can see from here that the best 2 predictor model has the predictor hits and C, R, B, I. And then we get their R squared, which is like 76%. And then we can also get their AIC, BIC score from it. So this is for the best 2 predictor model. So a simple exercise is to show the best 19 predictor model. So there's actually only one because we have 19 predictors in total. So the best 19 predictor model is to increase everything. And then we can see that the R squared is a bit higher than R squared for the best 2 predictor model, which is as expected because we use more predictors. And then next, we can extract the R squared by calling the R squared. So we can also guess the R squared from best 1 to best 7 predictor model. So we can see that it seems like the R squared increases with the number of predictors. And then the adjusted R squared is the highest when we use the best 7 predictor model. And then, so for the model summary, we also have the BIC and AIC score. So we can extract that information and make a plot for the best 1 to 7 predictor model. So that is what we do in this cell. We make a plot for the received sum of squares, the adjusted R squared, AIC, and BIC. So we can see that it seems like for adjusted R squared and AIC, we select the 7 predictor model. And then for BIC, we select the 6 predictor model. And then recall that in our lecture, we talk about the distinction between AIC and BIC. So in the AIC, we adjust by the number of predictors, and then the coefficient in front of it is 2. But for BIC, we also adjust for the number of predictors, and then the coefficient is log n. So as long as the symbol size is larger than 7, BIC always puts a larger realization on the number of predictors. So BIC tends to select a sparse model with a smaller number of predictors, which is also confirmed here. We can see that the BIC selects the 6 predictor model, while the AIC selects the 7 predictor model. So this is for the best success selection. And then the next is to implement the forward and backward selection. So for the forward selection, we also write a function called forward. So in this function, what we do is that we get all the remaining predictors in x that is not included in the input predictors. So the idea is that we have all the predictors in x, and then we pass in the current predictors in the model, and then we guess the remaining predictors. Say if we want to identify the best true predictor model, now we only have one predictor. So what it does is we first pass in the one predictor, and then we guess the remaining 18 predictors, and then we find within these 18 remaining predictors which one should be added to their current best one predictor model so that we can get the best true predictor model. So that is the logic for this function, where in the remaining predictors, we write a for loop and then fit a model using this process subset, where we incur the existing predictors plus this additional candidate's predictor. And then we get the model, fitted model, and the residual sum of squares. Then the next is we select the model with the lowest residual sum of squares. And then this is what we call as the best model. And then we count how long it takes to iterate through all the remaining predictors to get the best model, and then we return the best model. This is what we do for this forward selection. And then we can iterate through the best one predictor model to best 19 predictor models using this forward selection. So you can see that actually it's pretty quick for this code to identify the best model for all the possible numbers of the predictors. So when we want to find the best one predictor model, we need to process 19 models in total because each of the 19 predictors we need to compute to fit the model and compute the residual sum of squares. But once we have this one predictor model, to get the two predictor model, we iterate through all the remaining 18 predictors and identify which predictor has the lowest residual sum of squares. So in this step, we only need to fit 18 models. And also for the three predictor model, conditional on the two predictors selected, we iterate through the remaining 17 predictors and find which predictor added can most reduce the residual sum of squares. So in this step, we fit 17 models in total. So we follow the same logic and we can count how many models we need to fit if we want to find the best four predictor to best 19 predictor model. And then the next is we can see what is the best model selected by the forward selection. For the best one predictor and two predictor model. So for the best one predictor model, we can see that the predictor selected is the number of hits. And then for the best two predictor model, the predictor selected is hits and the additional one is the CRBI. So for these two predictors, we can see that it's the same as the two predictors selected by their best subset selection. So actually, we can do the same exercise for the best six predictor model selected by their best subset selection and forward selection. And then we can see that the top panel is for the best subset selection and then the bottom panel is for the forward selection. And if we compare their coefficients and their variables, they are exactly identical. So actually, for this data, the best one variable to six variable models are identical between their best subsets and forward selection. So the next is to implement the backward selection. The idea of the backward selection is that we start from the 19 predictor model and then we remove their least useful predictors one by one using the criteria that the removed predictor has the smallest impact on their residual sum of squares. So to do that, we write a backward function here. And then the idea is that we iterate through all their predictors and then we find a subset of the predictors with one predictor less than the current number of predictors. So basically, in this iteration, we remove one predictor at one time. And then we see which predictor removed would have the lowest impact on the residual sum of squares in the sense that it has the least increase in the residual sum of squares. So we select the model with one predictor less but has the lowest residual sum of squares. And then this is what we call as the best model and we return this best model. So we can use this function and then we iterate all the possible number of predictors. And we start from the 19 predictor model and then we find the 18 predictor model, 17 predictor model up to the one predictor model. So here, if we run this block of codes, then we can see that starting from the 19 predictor model, if we want to get the 18 predictor model, then we need to fit 19 models in total. So for each model, we remove one predictor from the 19 predictors. So there are 19 possible choices and then we need to fit 19 models in total. And then similarly for the next, if we want to find the 17 predictor model, then we start from the 18 predictor model and then we try to remove each one from the 18 predictors and see which one has the lowest impact on the RSS. So in this step, because we have 18 predictors to start with, so we need to fit 18 models in total. So that is the logic for the remaining. And then here we finish finding the 18 to 1 predictor models. So next, as we compare the 7 predictor models selected by the best subset selection, forward and backward selection. So what is interesting here is that once we reach to the best 7 predictor model, we can see some difference between these three subset selection approaches. So for the, this is for the best subset selection, we show the model, the parameter coefficient estimates. And this is for the forward selection and then for the backward selection. So if we compare these three sets of results, we can see that, so for the forward selection, it incurs the leak not, but the best subset selection does not. And then the best subset selection incurs this variable, chmrun, but the forward selection does not. And then further, if we compare with the backward selection, so for the backward selection, it does not incur the leak n. And then the backward selection incurs the variable cwalks. That does not incurs in their forward selection. And then if we compare their best subset selection and backward selection, so cwalk is in backward, but not in best subset. And then the variable chmrun is in the best subset, but does not incur this. It's not included in their backward selection. So you can see that if we select up to 7 predictors, we can see some difference between these three methods. So the takeaway message here is that the forward and backward selection is more computationally efficient, even though it may not identify the best possible combination of the predictors. But usually the gap is not that big. So that is how to implement their subset selection, and I hope this could be useful for you guys to complete the subset selection question in homework 2. And then the next is to see how we can implement the LASSO and REACH in Python. So that is for the next notebook. So in this notebook, then we incur the standard packages here. And then we also use the same data to predict the baseball players' salary. So similar as the first exercise, we remove the players without the salary information. And then here we use different methods to construct the dummy variables when the variables in the original data set is categorical. So the way to do that is we can use this model spec from the is.models library. And then in this, we can then pass in all the columns except for the outcome salary. And then we fit, call the fit, and pass in the current data frame with all the players with the salary information. Then we call the fit transform, and then we drop the intercept term. So in this case, we can also convert all the categorical variables into their dummies. So this function, this approach does the same job as what we do here, where we explicitly construct the dummies for those categorical variables. So for this approach, we don't have to specify which variables are categorical. So if you find this to be more convenient, then you can also use this in your homework. And then the next is to implement the ridge regression. So for the ridge regression, I recommend this approach is that we first standardize the variables. And then the idea is that the coefficients, once we standardize the variables, are comparable with one another. So the shrinkage parameter lambda can work in the same way for all the variables. So that is more recommended. Now we can then avoid the scenario where some variables is much smaller than some of the other variables. And then for the variables with a smaller scale, the coefficient may be larger, and we will run lasso and ridge. This variable with a smaller scale, the coefficient may be penalized more, and that is undesired. So to avoid this issue, we want to pre-process the variables so that they have mean zero and variance one. And then for this ridge regression, what we do is we use the SKL elastic net to fit both the lasso and ridge. Now we have another function, which is called the SKL elastic net SCB, which can help us select the optimal tuning parameter lambda. And then here, so for the lambdas, we specify it in this way. The whole idea is that the lambda can cover as large range as possible, so in the power is equally spaced. And then we divide by the standard deviation of the y, the salary, so that the lambda is accounts for the scale of the y. And then we pass in the x, which is the standardized version of the x, pass in y. And then if we specify L1 ratio as zero, then this means it's the ridge regression. And as a comparison, if we specify L1 ratio as one, then this is lasso. So for this alpha's argument, we pass in the lambdas, which are the grid of the tuning parameter. So here, we search 100 different lambdas in total. And then this solution array conditions the ridge regression coefficients for each possible lambda value. So here, we take the log, so that after we take the log, all the lambdas are equally spaced. And then we can plot the coefficients for each possible lambda. And then here, the x-axis is the minus log lambda. So in this case, then all the lambdas can be shrinked into the same range. And then for how to read this figure, for the rightmost case, it's the case where the lambda is very, very close to zero. And then their coefficients on the right would be very similar to their least square coefficients. And then for the leftmost point, then that is the case where the lambda is very big. So we have a huge shrinkage on the coefficient, and all the coefficients are getting very close to zero. So that is how we read this figure. And for different curves, it denotes the coefficient path for a specific variable. And then the next is, we can look at their coefficients for a specific lambda value. And we just randomly find one, which is at index 39. And then for this, the lambda value is like 25.53. And then we can get all their ridge regression coefficients. We can also compute we can also compute what is the corresponding beta value, the L2 norm for this beta vectors. So the L2 norm is 24.17. So to put this into context, we can also compare the L2 norm for the coefficients when their lambda is smaller. Say the lambda is like 0.24, which is 100 times smaller than this one. Then in this case, when lambda is much smaller, then we have a smaller realization for the beta, for the coefficients. And then in this case, you can see that the L2 norm for beta is way bigger, it's 160 as compared to the previous case where the L2 norm is just 24. So a larger lambda would lead to a smaller coefficient. So that is the takeaway from this comparison. And then for this lambda, it needs to be specified by the user. And we have learned in class that a good approach to define a good lambda is by cross-validation. And then to conduct cross-validation, what we do here is we need to first import a few packages. So we need to import this SKM, the model selection, so that we can specify the k-fold selection. So here we are interested in the 5-fold selection. And then definitely you can change the k from 5 to 10 to implement the 10-fold cross-validation. So if we want to implement the cross-validation, we also need to use two other functions. One is the standard scheduler. So for this, it's just an automatic approach to normalize their features. And then we use the sklearn.pipeline. So this pipeline is a function which can help us automatically implement the cross-validation. And then for the reach, what we do is we use the skl.elasticnextcv. And we specify all the lambdas we want to search. So these lambdas are identical to the lambdas in our first exercise. And then the next is the L1 ratio is equal to 0. So this is the case for the reach regression. If we specify L1 ratio as 1, then that is less so. And then CV specifies the k-fold, which is the output from the skm.k-fold. And then we have this scheduler where we specify their demean and standardize as true. And then we pass the reachCV and scheduler into this pipeline. And then we call the fit. Then we can get the optimal lambda from 5-fold cross-validation. And then we can visualize the result from the pipeline. So what we do here is we plot the cross-validation mean squared arrow for each possible lambda. So in the rightmost case, that is for the smallest lambda. For the leftmost case, that is for the largest lambda. And then we want to select the lambda with the minimum cross-validation arrow. So here, what we get is, so this is the optimal lambda selected. And we have this cross-validated arrow. And the arrow bar is calculated from their k-fold cross-validation. And then the vertical line denotes the optimal lambda selected by their cross-validation, which has the smallest cross-validation arrow. And then we can also output their coefficients for lambda in this case by calling the star coefficients. And then we can map this to their features here. And then we can get to know their coefficient for each variable. So this is for the reach regression. For the last one, that is pretty similar. The only difference is that we specified the L1 ratio as 1. And for the cross-validation, their cost is pretty much similar to the previous case. But the only difference is that there are two differences. The first is we have a different L1 ratio. And then the second one is, for LASSO, we can just specify the number of alphas, which is the number of lambdas we want to do the cross-validation. So here, we can specify 100. But for reach regression, we need to manually pass in the lambdas we want to do the cross-validation. So we cannot just simply specify the n alphas as 100 and run the reach regression. That doesn't work. So we need to specify the vector of the lambdas we want to do the cross-validation for reach. But for LASSO, it's more convenient. And then besides these two points, the others are pretty much the same. So we call the pipeline, pass in the scalar, which we want to do the standardization. And then this LASSO CV is what we have from the outputs from the previous function. And then we fit the pipe CV. And then we can get the optimal lambda from cross-validation of LASSO. And then the next is we can also plot the coefficients from LASSO. And then we call this SKL LASSO path. And then we can show there are standardized coefficients. So you can see here that also the x-axis is minus one lambda. So for the right part, the lambda is very small. And then the coefficient is similar to the least square coefficients. And for the leftmost case, then the lambda is very big. And then we have lots of shrinkage. So you can see here the x-axis has a different range than the previous case where we search for a larger range. So if we want the LASSO to also search for a larger range, then we can also manually specify the lambdas we want to do the cross-validation. But here we can still observe the interesting pattern. So for LASSO, we can shrink the sum of the coefficients to exactly zero. Like here, the yellow one is exactly zero. And that is unique. So that is the feature for LASSO but not for REACH. And then the next is we can see that some of the coefficients are not monotonic. So as the lambda decreases from the left to the right, some of the coefficients may first decrease and then increase. So it works for this orange one and also works for this light blue one. So the coefficient is not monotonic with the lambda. And then next, we can also plot the cross-validation arrow for all the possible lambdas and plot the optimal lambda in LASSO. So the optimal lambda here is like 3.14. So here we do the cross-validation. So the vertical, the solid line denotes the cross-validation arrow. And then the arrow bar is computed from the five-fold cross-validation. And then this black one is the one with the lowest cross-validation arrow. And then the lambda value is 3.14. And then we can also plot the coefficients by choosing with the optimal choice of the lambda. So that is the Return's lambda dot coefficients. And then we can get the optimal coefficients. So we can see that here some of the coefficients are exactly zero. So in this case, the LASSO performs the variable selection and these variables are not selected by this optimal lambda. So basically, that is R for the realization. And then if you want to implement the elastic net, the only change that you need to make is for this L1 ratio, then you want to specify a different value than 0 and 1. You may want to select a value that is between 0 and 1, like 0.3, 0.7. And then a more complicated thing is how to do the cross-validation when the L1 ratio could vary. So basically, this ratio also needs to be cross-validated. So if you are interested, then you can also try to implement the cross-validation for elastic nets. But that is not required for the second homework. So I hope this example course can help you to finish the second homework. So this Zoom lecture is a bit short. And then I want to leave you some time to also try to run this course by yourself and try to digest this course. And I also want to give you some time to prepare for the presentation on Wednesday. So I will send an announcement about the instruction for the presentation by the Monday's lecture time. So please check the cameras for the instruction. And that's all for this lecture. Let me know if you have any questions. Thank you very much.
