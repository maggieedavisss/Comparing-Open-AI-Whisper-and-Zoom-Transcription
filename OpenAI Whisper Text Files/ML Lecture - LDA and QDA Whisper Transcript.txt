Hi, everyone. I hope you all had a wonderful weekend. Today, we are going to learn LDA and QDA. We are going to learn how to estimate the parameters in LDA and QDA. And we are going to learn the zero boundaries of LDA and QDA. Both LDA and QDA are examples of the generative methods. So what are the generative methods, as we learned in our last lecture? Instead of directly modeling the probability of a class given the features for the generative methods, we model the data-generating process and then we use the data-generating process to help us make the decision and to help us predict their labels. And more specifically, for the classification problem, for the generative methods, what we do is that we first model the probability of the features. Like in our iris example, in our last lecture, we modeled the distribution of the sepal length, sepal width, petal length, and petal width for each iris type. That is the probability of X given the Y. We also model the probability of each class, each type of iris. And then we apply the Bayes theorem to convert the probability of features given each class and the probability of class to the probability of class K given their label. And then we use the probability of a class given label to help us make classification. And then at the end of last lecture, we briefly mentioned how to estimate the probability of the features given each iris type and the probability of each iris type, Y equals to K. In today's lecture, we're going to learn this in detail. So first, let's start from the simple one, where we want to estimate the probability of each class. We use pi K to denote the probability of class K. And then other estimation of the probability is pretty simple. We just count the fraction of the training samples that are in class K. And then basically this is to count how many labels in class K and we divide by the total number of samples. In our iris data, we have 50 samples from each of the three classes of vesicular, setosa, and majinika. So given this information, then the probability of each class is equal to one third, which would be 50 divided by the total number of samples, which is 150. So then the pi K is equal to one third for each of the three types. So the estimation of the class probability is simple. The more challenging part is how can we estimate the probability of the features given each iris type. And for LDA, we model the probability of features given iris type by a multivariate normal distribution with mean mu k and the covariance matrix as sigma. We have four features in total. So now let's look at how we can estimate the mean of the simple length, simple width, simple petal length, and petal width. Conceptually, the way to do that is we calculate the sample average of each of the four features using the samples for each class. So as an example, for the setosa, then what we do is we take all the 50 samples of type setosa, and then we calculate their average simple length, average simple width, average petal length, and average petal length. Graphically, this would just be equal to the black dots in the box plot for each one for setosa. And then mathematically, what we do is we take all the samples of setosa, and here suppose k is equal to setosa, and then we calculate their sample average of this 50 samples features. And then the more challenging one is how can we estimate their covariance matrix in their multivariate normal distribution. Recall that in the LDA, we make a critical assumption that their covariance matrix of their features is equal for all the three types. And then in this covariance matrix, the diagonal terms are the variance of each of the four features, and the off-diagonal terms are equal to their covariance between any two features. And then we assume that the variance and covariance are identical across all their three classes. And then the first question is, how can we estimate just one single covariance matrix across the three classes? And another question is, what if their covariance matrix are different across classes? So for those two questions, how can we solve that? And then to consider these two questions, let's first look at the second one. How can we estimate, so what if their covariances are different across the three classes? So after we address the second question, then we can, it's easier for us to address the first question. How can we estimate just one single covariance matrix across those three classes? So to address the second question, what if their covariance matrices are different across the three classes, are different between the Sopoza, Vesicolor, and Virginica? So in that case, then it's possible to consider another generative method, that is the QDA. And then in the QDA, then we allow their covariance matrix between the features to be different. So in this case, then we index their covariance matrix by k, which means we allow the sigma k to vary across each of their three types. So how can we estimate their covariance matrix? So how can we estimate their covariance matrix of the Sopoza? The idea is that for their diagonal terms of this covariance matrix, we just calculate their covariance of each of their four features using the samples of Sopoza only. And then for their off-diagonal terms, their covariance between any two features, then we also use their Sopoza samples only. Mathematically, what we do is that we suppose we want to estimate their covariance matrix of Sopoza. Then what we do is that we take 50 Sopoza samples, and then we calculate their deviation from their mean features. The mean features is identical to the mu k we estimated just now. And then we take this difference, and then we take its transpose. So in this case, as we have four features, when we use this operator, we multiply the difference between each sample's feature and the mean, and we multiply by its inverse. Then we get a 4x4 matrix, which would be the variance-covariance matrix of Sopoza. And then we sum this object across all the 50 Sopoza samples, and then we adjust by their sample size. Here we adjust by nk-1, and in the Sopoza case where nk is 50, so we adjust by 1 over 50-1, which is 1 over 49. And then we get their sample covariance matrix. And then in R or in Python, we can use the command covariance to compute their covariance matrix, given their samples of their features. And then here, this picture shows their samples of Sopoza, and the dot shows their feature value for each observation. And then based on this dot, we can get a rough sense of how these four features are correlated with one another. Basically, you can see that between the sample length and the petal length, it seems that if the sample length is larger, then the corresponding petal length is larger. And also for the sample length and sample width, it seems like if the sample length is larger on the right region, then the petal width is also larger. So those ovals are shifted to this northeast diagonal direction. So basically, you can see that it seems like these features are positively correlated with one another. And if we look at their covariance matrix, and indeed, we confirm that you can see that all those numbers are positive, not only the diagonal terms of variance are all positives. And the off-diagonal terms are also positives, which means the correlation between any two features is positive. So we can do that for Sopoza, and we can repeat this procedure for vesicolor. We only take vesicolor samples, and then we use the same procedure. For each observation of vesicolor, we calculate the difference between vesicolor's feature value and its estimated mean, and we multiply by its transpose. We sum over all the samples of vesicolor, and then we adjust by 1 over the sample size minus 1 to get the covariance matrix of vesicolor. Now we can also see that based on the feature value of the vesicolor samples, it seems like if one feature is larger, the other features are also larger. These features are positively correlated. This can also be confirmed by this variance-covariance matrix computed from the vesicolor samples. All the numbers here are positive. An interesting observation is that if we compare the variance-covariance matrix of vesicolor versus Sopoza, you can see that the variance of Sopoland is larger than the variance of Sopoland for Sopoza. If we look at other features like petholength and petholwidth, the variance of Sopoza is smaller than that of the vesicolor. Basically, this shows that vesicolor has larger variance and covariance as compared to the samples of Sopoza, which can then be considered as useful information if we want to use the QDA to separate between two classes. So far, we have talked about what if the variance-covariance matrix is different across classes, and then we still haven't addressed the question that if the variance-covariance matrix is assumed to be identical across the classes, then how can we estimate just one single variance-covariance matrix across classes, which is an important object to be estimated in LDA. In fact, the estimation of the variance-covariance matrix in LDA uses the object that is estimated from QDA. So here, basically, we do the same thing as QDA. We estimate the variance-covariance matrix for each class, and then we average them together across the three classes. So how do we average them? We weigh by roughly the fraction of the sample size in each class. So in our iris example, what we do is we estimate the variance-covariance matrix of Sopoza, vesicolor, and virginica. And then we average this three variance-covariance matrix by the weight of the fraction of observations in Sopoza, the fraction of observations in vesicolor, and the fraction of observations in virginica. And then we adjust by the degree of freedom. Here, as there are 50 samples in each of the three classes, so N Sopoza is equal to N virginica, equals to N vesicolor, equal to 50, and then the N, the total sample size is 150. So basically, in our example where the number of samples in each class is identical, then the estimated variance-covariance matrix in LDA would just be a simple average of the variance-covariance matrices of all the three classes. But this is just a special case when the number of samples is identical across the three classes. In some cases, we may have very imbalanced labels. For example, in the credit card example, most of the customers do not default, and only a small fraction of customers default. So in that example, when we estimate just one single-covariance matrix in LDA, we would have a much larger weight of the variance-covariance matrix for the non-default customers, because they count as a much larger fraction as compared to the default customers. So basically, this is just a measure to weigh different variance-covariance matrices together, and if one class has a much larger sample, then we will assign a higher weight in this calculation. And the intuition is that if this class has a larger sample, then we think that their estimation precision of the corresponding variance-covariance matrix is better. It can be estimated more precisely, which would then be more useful. So we assign it with a higher weight in the calculation of just one single variance-covariance matrix in LDA. But definitely, if we look at the samples of the feature value for LDA and QDA, the blue dots are suppose-size observations, the virginica samples are the green dots, and the versicolor samples are the green dots. And then you can actually see that from these figures, especially between the suppose-size and the other two, they are separated, and they seem to have different variance and different mean values. The variance could roughly be measured by the width of the oval here and the height of the oval here. And for different classes, the shape of the oval seems to be different, which is a sign that the variance-covariance matrix is different. And then the LDA is definitely making a strong assumption to assume that their covariance is identical across classes. So this is how we estimate the feature mean and feature covariance in the multivariant normal distribution in the probability of feature given each class. And then now let me summarize a bit of LDA. So for LDA, basically, for each class K, setosa, virginica, and versicolor, we model the feature probability given each class by a multivariant normal with mean mu K and variance-covariance as their sigma. And both are estimated from the procedures that we described just now. So then for each class, we have one normal distribution, and we also have the probability of each iris type. Then how can we use the LDA to make prediction? What we do is we want to estimate the probability of each type given their iris features. So we come with a new sample with the four feature values, their simple length, simple width, petal length, petal width. And we want to use these feature values to make prediction that which of the three iris types is the most likely one. So to answer this question, based on what we have in the LDA, we use the Bayes theorem to calculate the probability of each iris type given the value of four features. And what we do is that the probability of class K given feature value is equal to the joint divided by the probability of their features. And then for the numerator, we can write it as the probability of class K multiplied by the probability of feature value given the class K. So for the denominator, we can separate the denominator as the sum of the joints across the three iris types. And for each iris type, type J, then we also write the joints of the feature value and type J as this product of two probabilities. One is the probability of feature value given type J and the other one is the probability of type J. And then we sum over the three J's, the three classes in the iris example. So now you can see that in LDA, once we estimate the probability of feature given class K and the probability of each class, we can then use this to plug this into this blue component. And then we can obtain the probability of class K given the value of four features. The next would be to compare the probability of each iris type and then to predict iris type label for this observation. And then let's also have a summary of the QDA. So for QDA, it's pretty similar. The only difference is that we estimate the probability of the feature given class K as the multivariate normal with sigma K. And then we plug this feature probability and the class probability into the blue component and we obtain the probability of each iris type given the feature value. So once we have the probability of each class given the feature value, based on what we have learned in our Monday's lecture, we can apply the Bayes classifier and output the class with the highest probability. And this procedure can minimize the misclassification error rate. So that would be the decision rule. And we also introduced a concept which is called the decision boundary. So the decision boundary means this is the set of X where the probability of two classes are identical. So what does that mean? Let's look at a graphical example. So suppose we just have two features, X1 and X2. And then we have in this classification problem, we have three classes. The yellow one, blue one, and the green one. Here there are dots in those observations. And suppose in this simulated example, we know the true decision boundary. And this means this is how the data is generated. And basically, this is computed from the data training process. And this shaded yellow color is the region where the probability of yellow is the highest. And then this shaded blue region is the region where the probability of blue is highest. And green is the region where the probability of green is the highest. And then for the solid blue lines here, say for this one, it means that in the data training process, the probability of green and yellow is identical. And on this line, basically it's equally fine to make the prediction as yellow and as green. So both would make the same fraction of mistakes when we do the classification problem. And similarly for this solid blue line, the probability of green and blue is identical. And similarly for this one, the probability of yellow and blue is identical. An important property that we need to consider is that in LDA, what is the zero boundary? So what is the shape of the X? Where on this set of X, the probability of any two classes is identical. And here, an important property is that the LDA has linear decision boundaries. So the boundary where the set of points in which two classes do just well is a linear function in X. So this decision boundary is to separate class 1, class 2, or class 2, class 3, or class 1, class 3. And for each decision boundary, it means that the probability of class K and class J for two different K and J is identical. And here, this shows what is obtained from the LDA. And the LDA can return a boundary, and on this boundary, the probability of green and yellow is identical. The probability of blue and green is identical. The probability of green and blue is identical. So this boundary is linear in X1 and also linear in X2. That is what we meant by the linear decision boundary on this boundary. So this boundary is the linear function of X. So graphically, it makes sense that on this line, the probability of two classes are identical and is linear in X1 and X2. But why do we have such a property? So why does the LDA have the linear decision boundary? So let's consider two classes, K and L. So following the definition, if X satisfies that the probability of K and L are identical, then X is on the decision boundary of K and L, where we are indifferent between making predictions as type K or type L. So if X satisfies this property, then we can take the log of both probabilities. And it should be this X also satisfies that the log of probability equals K given the features should equal to the log probability of L given their feature value. So taking the log is just a one-to-one mapping. And then we can see what is the set of X that can satisfy this equation. Basically, an important observation is that with some algebra, we can show that the log probability is equal to some constant C that does not depend on the class, plus a delta function that depends on class K. And this function is a function of X, which varies with K. More specifically, this function takes such a form. This form, basically, depends on the probability of class K, and also depends on the feature mean and the variance matrix of the features, and also depends on the X. So you can see that this delta is a linear function in X. So if the delta is a linear function in X, then why does the decision boundary should be linear? So here is the logic. The decision boundary that separates class K and L is a linear function of X. Then we can use this priority and then plug in to the set of X that satisfies this equality. So we plug the C plus delta K of X into this object, and then we plug C plus delta L of X into this object. And then C does not depend on class K and L. So this term, C, can be cancelled out. This constant can be cancelled out. And we are left with delta K of X minus delta L of X. Then we can plug the definition of delta K of X and delta L of X into this difference. And with some algebra, we can show that the difference is equal to this term. Then the next step is we want to simplify the notation a bit. We aggregate the first four terms into C0. So C0 depends on class K and class L, but this first four terms does not depend on X. And then we have a second term. So this second term is X transpose, an object that does not depend on X. So this object does not depend on X. Sigma inverse times mu K minus mu L can be written as C1. So you can see that C0 plus C1 X is a linear function in X. And then we consider what are the X such that C0 plus C1 is equal to 0. You can see that all the X that satisfy this equation is linear in X, which explains that here in this slide, we can see that the boundary where two classes are identical is linear in X1 and linear in X2. Oh, so you may wonder, how can we obtain the expression of delta K of X? And it seems like it's because delta K of X is linear in X, so that we can obtain that the same boundary where two class probabilities are equal is linear in X. So how can we get the expression of delta X? And the way that we get the expression of delta X comes from the fact that the probability of the feature value given Y is a multivariate normal distribution. So this slide has a lot of math, but for those that are interested, then we can try to digress a bit what is going on here. But for those who think that math is too intimidating, so basically this page is not required for the homework and for the exams of this course. But this is just a little bit more details for those who are curious about why the delta X is a linear function of X. This comes from the fact that the multivariate normal distribution of feature given class K can be written in this form. So if we consider X as just one-dimensional, then basically this expression is just equal to 1 over the square root of 2 pi. So P here is the feature dimension. And in our example where we have four features in total, P is equal to 4. And this one means the determinant of the variance-covariance matrix, and we take the square root of it. In the univariate feature case, this object is just equal to the standard deviation of the feature value. And then this is the exponential to minus one-half times the X minus the estimated mean of class K multiplied by the inverse covariance matrix and multiplied by the X minus mu K. And in the univariate case, basically we can write this as the square difference between X and the mean and divided by the variance of X. So you can verify that in the univariate dimension case, this is back to the standard here of the normal distribution. So we just take this as given. This is the multivariate normal. And then we use this expression to show that the mu K of X is linear in X. So the way we do it is that we then go back to the definition of the log probability of Y equals to K given X. So we can write this by the base theorem as the joint probability of X and Y divided by the probability of X. So when we use the log operator, the log of two ratios is just equal to the difference between two logs. So we can separate that into the log of the joint probability of X and Y multiplied by the log of the probability X. And then the joint probability of X and Y can then be further decomposed into the probability of class K multiplied by the probability of feature given class K. We then further use the probability that the log of the two products is equal to the sum of the two logs to separate this object into the two here. Then the next step is we use the definition of each object. So for the first one, the probability of class K is equal to the pi K hat. In our RS example, it's one third. So it's the log one third. And for the second object, then this would be the log of this density function. And then the log of the first one is just some constants that does not depend on their class K. And then for the second one, the exponential to minus one half object can be written as this one because the log of the exponential is just equal to the minus one half times this quadratic form of X. And then the first term does not depend on class K. So it's absorbed into this C prime that does not depend on K. And also for this minus log feature probability, this also does not depend on class K. So we can also absorb this term into this C prime. So basically what we get here is that for the first and the C prime, we combine them together. So basically for this object, we further separate the components that depend on K and those that do not depend on K. And then those depend on K is summarized into this delta K of X. So we can see which terms depend on K. For sure, the log of pi K depends on K. And also for this one, we can also further separate this into the parts that depend on K and the parts that do not depend on K. So parts that do not depend on K is this minus one half X transpose sigma inverse times X. So this term does not depend on K. And all the remaining components in this object depends on K, which are equal to the second and the third terms here. So you can see that the objects of X square does not depend on K. So it's combined into another constant C. And then we are only left with the component that is linear in X. So you can see that even though the density is a quadratic function in X, we have this X transpose sigma inverse X. But this square term does not depend on K. So it's shared, it's identical across the three classes. And this does not affect the shape of the certain boundary. So we can combine it into some constant term. And the term that depends on class is only linear in X. So this explains why the delta K of X is the linear function of X. And when the delta K of X is linear in X, then what we have on the first slide, the certain boundary that separates class K and class L is linear in X. So graphically what this means is that the certain boundaries that separate the two classes, say to separate between setosa and versicolor, is linear in their total width, total length, simple length, and simple width. So it's a linear function of the four features. And on this boundary, the probability of setosa and versicolor is identical. So basically here we have discussed in detail why the LDA has a linear discern boundary in X. And if you have any questions, please let me know. But what if the QDA? So for QDA, the fundamental difference is that QDA does not have a linear discern boundary. And the reason is that the log of the probability of Y given X is equal to C plus this delta X. And delta X is a quadratic function in X, as shown here. So it's quadratic in X. And why do we have this quadratic term? So basically this is because, as we have the math here, this object is indexed by K. So we, which means the X transpose sigma KX depends on class K. So we cannot combine this term into a constant that does not vary with class anymore. And we need to put this term into delta K of X. So this delta K of X, because of this quadratic term, is a quadratic function in X. And when the delta K is no longer linear in X, it's quadratic in X. This makes the discern boundary of QDA to be quadratic. And this can then make us to compare the discern boundary and the tradeoff in LDA and QDA. So suppose we just have two classes, the blue one and the green one. And then the discern boundary of QDA, you can see here, this is a quadratic function in X1 and quadratic function in X2. And on this boundary, predicted by QDA, the probability of blue and yellow are identical. Similarly here, you can see that this green curve, predicted by QDA, is quadratic in X1 and X2. And on this boundary, the probability of two classes are identical. And then for LDA, this is linear in X1 and X2. And then we may consider what would be the tradeoff between the LDA and QDA. So QDA has more parameters because the covariance matrix is indexed by X. And the LDA has fewer parameters because the covariance matrix is not indexed by X. So LDA is less flexible than QDA. So when we have a small sample size where we care a lot about the variance, then QDA, because it's less flexible, it has a smaller variance. So this may be more preferable. And then if we have a large sample size, when the variance is not a big concern, then QDA has more parameters and is more flexible. So the QDA may be preferable. But this also depends on what the true discern boundary is. If the true discern boundary is linear, then the LDA would be better. But if the true discern boundary on the right case, this purple one, is non-linear, then the QDA can approximate the true discern boundary better. Then the QDA would have smaller classification error rates than LDA. So here, I just wanted to make a side comment. In LDA, we also use the sigma k to estimate the sigma matrix. So it seems like in LDA, we also have a procedure where we estimate a different covariance matrix for a different class. Then you may wonder, why do we say that the LDA has fewer parameters? The fundamental reason here is that in LDA, fundamentally, when we classify, we classify based on the probability of Y given X. And when we predict based on the probability of Y given X, we use the delta k X where the sigma does not depend on k. So the information or the parameters that are used to make the classification does not depend on k. So this means the parameters used to make classification does not depend on k. So the number of parameters that are relevant for classification is less than the information or the estimated number of parameters in QDA used to make classification. So when the number of parameters that are relevant to make classification is fewer in the LDA case, then the LDA is less flexible. Okay, so this is the comparison between LDN and QDA. And you may also wonder, how do we compare their LDA with their logistic regression? So for LDN and logistic regression, there are also some similarities between the two. And there are also some differences. The similarity between the two is that both LDA and logistic regression produce their linear discern boundary. So we have talked about their linear discern boundary for LDA, but why their logistic regression also has a linear discern boundary? The reason is that in logistic regression, what model do we impose? We impose a logit model. In the logit model, the right-hand side is a linear function of X and the left-hand side is a log of. So what is the discern boundary? The discern boundary is the set of X such that three class probabilities are equal and both equal to 0.5. In other words, the log of the probability of 1 is equal to the log of the probability of 0 given X. And this object is just equal to the log of and equal to beta plus beta 1 X. So you can see that all the X that satisfy beta 0 plus beta 1 X is equal to 0. It's linear in X. And it looks like a similar form as the LDA case where the set of X that satisfy C0 plus C1 X is equal to 0. So we can see that the set of X where two probabilities are equal are all linear in X. So that's why their logistic regression also has a linear discern boundary. But the way LDA and logistic regression obtain their linear discern boundary is different. The LDA uses the generative method, but the logistic regression imposes a logit model and uses the maximum likelihood estimator to obtain the linear discern boundary. So the estimation procedure is different. In certain cases, the LDA and logistic regression produce similar results, but this is not always the case. We may consider when the LDA is more preferable and when the logistic regression is more preferable. So LDA can be better if the assumption of their normal distribution is reasonable. For example, in the Iris example, where the feature distribution is roughly normal, then we expect that in this case the LDA could be better. But if their feature distribution is very far from normal, then the assumption of LDA is violated. In this case, it's possible that the logistic regression is better. So this is all I want to talk about for this lecture. And at the beginning of next lecture, I will briefly review the discern boundary of LDA, QDA, and logistic regression and answer your questions. We will also compare the LDA, QDA, with their k-nearest neighbors. And we will see some examples when each of those methods perform better.
